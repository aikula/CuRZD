{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оглавление \n",
    "\n",
    "* [Анализ данных и визуализация информации](#1)\n",
    "\n",
    "\t* [Подготовка данных](#1-1)\n",
    "\n",
    "* [Разведывательный анализ данных EDA](#2)\n",
    "\n",
    "\t* [Общая информация](#2-1)\n",
    "\n",
    "\t* [Гистограммы](#2-2)\n",
    "\n",
    "\t* [Визуализация средствами pandas](#2-3)\n",
    "\n",
    "\t* [Boxplot](#2-4)\n",
    "\n",
    "\t* [Парные матрицы рассеяния](#2-5)\n",
    "\n",
    "\t* [Корреляция](#2-6)\n",
    "\n",
    "\t* [Вариация ](#2-7)\n",
    "\n",
    "\t* [Группировки](#2-8)\n",
    "\n",
    "* [Обработка выбросов](#3)\n",
    "\n",
    "\t* [Удаляем выбросы](#3-1)\n",
    "\n",
    "* [Нормирование данные](#4)\n",
    "\n",
    "\t* [MinMaxScaler](#4-1)\n",
    "\n",
    "\t* [StandardScaler](#4-2)\n",
    "\n",
    "\t* [RobustScaler](#4-3)\n",
    "\n",
    "\t* [Различия в способах нормирования](#4-4)\n",
    "\n",
    "* [Кластеризация](#5)\n",
    "\n",
    "* [Кластеризация методом Kmeans](#6)\n",
    "\n",
    "\t* [Метод локтя](#6-1)\n",
    "\n",
    "\t* [Метод Коэффициента Силуэта:](#6-2)\n",
    "\n",
    "\t* [Дешифровка анализа](#6-3)\n",
    "\n",
    "\t* [Визуализация результатов анализа](#6-4)\n",
    "\n",
    "* [DBSCAN](#7)\n",
    "\n",
    "* [Ассоциативные правила](#8)\n",
    "\n",
    "\n",
    "\n",
    "# Анализ данных и визуализация информации<a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "## Подготовка данных<a class=\"anchor\" id=\"1-1\"></a>\n",
    "\n",
    "Ниже приведен код, который упрощает повторную обработку датасета. Данный набор данных мы будем использовать дальше в учебных целях.\n",
    "\n",
    "**Необходимо присвоить переменной `sql_pass` значение пароля для подключения к базе данных**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "\n",
    "engine = sqlalchemy.create_engine(\n",
    "                \"mysql+pymysql://root:%s@159.69.219.206:3307/rzd\" %(sql_pass), encoding='utf8', convert_unicode=True\n",
    "            )\n",
    "\n",
    "with engine.connect() as session:\n",
    "    df=pd.read_sql('SELECT * FROM ku_asrb LIMIT 1000', con=session)\n",
    "\n",
    "columns_date=['Дата события', 'Дата создания события в системе AC PБ', ]\n",
    "columns_float=[ 'Размер возмещенного ущерба (тыс.руб.)',\n",
    "               'Итоговый суммарный ущерб (тыс.руб.)']\n",
    "columns_int=['Погибло всего', 'Погибло пассажиров',\n",
    "       'Погибло сторонних', 'Погибло прочих',\n",
    "       'Получили тяжкие телесные повреждения: сотрудников ОАО \"РЖД\"',\n",
    "       'Получили тяжкие телесные повреждения: пассажиров',\n",
    "       'Получили тяжкие телесные повреждения: сторонние',\n",
    "       'Получили тяжкие телесные повреждения: прочие', 'Ранено легко', \n",
    "       'Путь общего/необщего пользования',\n",
    "       'Номер пути', 'Километр', 'Пикет', 'Общее время задержки']\n",
    "columns_time=['Время полного перерыва движения', 'Время расстройства маневровой работы',\n",
    "              'Количество задержанных поездов']\n",
    "\n",
    "def ddate(s):\n",
    "    try:\n",
    "        res=pd.to_datetime(s, format='%d%b%Y:%H:%M:%S')\n",
    "    except:\n",
    "        res=np.NaN\n",
    "    return res\n",
    "\n",
    "for i in columns_date:\n",
    "    df[i]=df[i].apply(lambda x: ddate(x))\n",
    "    \n",
    "\n",
    "for i in columns_float:\n",
    "    df.loc[df[i]=='.', i]='0'\n",
    "    df[i]=df[i].astype(float)\n",
    "    \n",
    "for i in columns_int:\n",
    "    df[i].fillna('.', inplace=True)\n",
    "    df.loc[df[i]=='.', i]='0'\n",
    "    df[i]=df[i].astype(int)\n",
    "    \n",
    "def time_to_sec(s):\n",
    "    s3=s.split(':')\n",
    "    if len(s3)<3:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(s3[0])*60*60+int(s3[1])*60+int(s3[2])\n",
    "    \n",
    "for i in columns_time:\n",
    "    df[i]=df[i].apply(lambda x: time_to_sec(x))\n",
    "    \n",
    "df['Время до регистрации']=df.apply(lambda x:(x['Дата создания события в системе AC PБ']-x['Дата события']).total_seconds(), axis=1)\n",
    "\n",
    "df.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разведывательный анализ данных EDA<a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "Разведочный анализ данных (англ. exploratory data analysis, EDA) — анализ основных свойств данных, нахождение в них общих закономерностей, распределений и аномалий, построение начальных моделей, зачастую с использованием инструментов визуализации.\n",
    "\n",
    "## Общая информация<a class=\"anchor\" id=\"2-1\"></a>\n",
    "\n",
    "Выберем несколько колонок для примера\n",
    "\n",
    "```python\n",
    "col=['Итоговый суммарный ущерб (тыс.руб.)', 'Общее время задержки',\n",
    "       'Количество задержанных поездов', 'Время до регистрации']\n",
    "df[col].describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гистограммы<a class=\"anchor\" id=\"2-2\"></a>\n",
    "\n",
    "Построим гистограммы.\n",
    "\n",
    "```python\n",
    "df[col].hist(figsize=(15,5), bins=40);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем события выше некоторого уровня, чтобы более внимательно их изучить.\n",
    "\n",
    "```python\n",
    "df[df['Общее время задержки']>30]\n",
    "```\n",
    "\n",
    "Строки с большими значениями (выбросы), могут мешать рассмотреть тенденции в основном массиве данных. Их стоит часто удалять или заменить на более приемлемые значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация средствами pandas<a class=\"anchor\" id=\"2-3\"></a>\n",
    "\n",
    "Расширим наши знания о построении графиков и визуализации информации.\n",
    "\n",
    "- `color` - определяет цвет графика.\n",
    "- `legend=True` - разрешает вывод легенды графика. Значение False подавит вывод.\n",
    "- `.set_ylabel`- задает подпись оси Y\n",
    "- `.set_xlabel` - задает подпись оси X\n",
    "- `.set_title` - задает заголовок\n",
    "- `figsize=(15,5)` - задает размер графика по ширине и высоте\n",
    "\n",
    "```python\n",
    "ax=df[df['Общее время задержки']<15]['Общее время задержки'].hist(figsize=(15,5), bins=40,  color='r');\n",
    "ax.set_ylabel('Частота')\n",
    "ax.set_title('Общее время задержки');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже мы просуммируем все задержки по времени по месяцам и построим график. Изначально мы создадим копию DataFrame и установим индекс дату события методом `.set_index()`. Для выполнения суммирования мы будем использовать метод `.resample()`. Где параметр:\n",
    "- `W` - неделя\n",
    "- `M` - месяц\n",
    "- `Q` - квартал\n",
    "- `Y` - год.\n",
    "\n",
    "```python\n",
    "dfM=df[['Общее время задержки', 'Дата события']].copy()\n",
    "dfM=dfM.set_index('Дата события')\n",
    "dfM=dfM.resample('M').sum()\n",
    "ax=dfM.plot(legend=False, figsize=(15,4), color='r')\n",
    "ax.set_ylabel('Время')\n",
    "ax.set_title('Общее время задержки');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете также строить и другие графики:\n",
    "- `bar` - столбчатая диаграмма.\n",
    "- `hist` - гистограмма.\n",
    "- `box` - ящик с усами.\n",
    "- `kde` или `density` - график плотности\n",
    "- `scatter` - матрицы рассеяния\n",
    "- `hexbin` - альтернатива диаграммам рассеяния\n",
    "- `pie` - круговая диаграмма.\n",
    "\n",
    "Подробнее и примеры по [ссылке](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html).\n",
    "\n",
    "Пример\n",
    "\n",
    "```python\n",
    "dfM.plot.bar(legend=False, figsize=(15,4), color='r');\n",
    "ax.set_ylabel('Время')\n",
    "ax.set_title('Общее время задержки');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot<a class=\"anchor\" id=\"2-4\"></a>\n",
    "\n",
    "Диаграммы размаха («ящик с усами») (Box and Whisker Plot или Box Plot) – это удобный способ визуального представления групп числовых данных через квартили.\n",
    "\n",
    "Прямые линии, исходящие из ящика, называются «усами» и используются для обозначения степени разброса (дисперсии) за пределами верхнего и нижнего квартилей. Выбросы иногда отображаются в виде отдельных точек, находящихся на одной линии с усами. Диаграммы размаха могут располагаться как горизонтально, так и вертикально.\n",
    "\n",
    "![Ящик с усами](https://3.bp.blogspot.com/-sqSGopnp0lo/Uvu_wl_dPQI/AAAAAAAAAgs/F2DBOSdfiU4/s1600/boxplot.PNG)\n",
    "\n",
    "Виды наблюдений, которые можно сделать на основе ящика с усами:\n",
    "- Каковы ключевые значения, например: средний показатель, медиана 25го перцентиля и так далее.\n",
    "- Существуют ли выбросы и каковы их значения.\n",
    "- Симметричны ли данные.\n",
    "- Насколько плотно сгруппированы данные.\n",
    "- Смещены ли данные и, если да, то в каком направлении.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(data=df[col], palette='rainbow', orient='h');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удобнее рассматривать не нормированные данные на отдельных графиках.\n",
    "\n",
    "```python\n",
    "sns.boxplot(data=df[df['Время до регистрации']<100000]['Время до регистрации'], palette='rainbow', orient='h');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парные матрицы рассеяния<a class=\"anchor\" id=\"2-5\"></a>\n",
    "\n",
    "```python\n",
    "sns.pairplot(df[col],height=3);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Укрупненный срез.\n",
    "\n",
    "```python\n",
    "sns.jointplot(x='Общее время задержки', y='Итоговый суммарный ущерб (тыс.руб.)', data=df, kind='scatter');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Корреляция<a class=\"anchor\" id=\"2-6\"></a>\n",
    "\n",
    "При разведочном анализе стоит посмотреть на взаимосвязь между переменными. А именно, на корреляцию.\n",
    "\n",
    "```python\n",
    "df[col].corr()\n",
    "```\n",
    "\n",
    "И построить тепловую карту для лучшего восприятия.\n",
    "\n",
    "```python\n",
    "corr = df[col].corr()\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(corr[(corr >= 0.3) | (corr <= -0.3)],\n",
    "            cmap=\"RdBu_r\", vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 8}, square=True);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант тепловой карты с группировкой по наиболее тесной взаимосвязи.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(14,14))\n",
    "sns.clustermap(df[col].corr())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариация <a class=\"anchor\" id=\"2-7\"></a>\n",
    "\n",
    "Помимо корреляции надо обращать внимание и на размах величины, а именно на ее коэффициент вариации.\n",
    "\n",
    "```python\n",
    "from scipy.stats import variation\n",
    "pd.DataFrame(variation(df[col]), index=col)\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более наглядно об идее.\n",
    "\n",
    "```python\n",
    "df2=(df[col]-df[col].min())/(df[col].max()-df[col].min())\n",
    "df2.plot(figsize=(17,5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Группировки<a class=\"anchor\" id=\"2-8\"></a>\n",
    "\n",
    "При формировании гипотез полезно посмотреть и на группировки по различным качественным признакам, понять как они влияют на показатели.\n",
    "\n",
    "```python\n",
    "df.groupby('Код дороги')[col].mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка выбросов<a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "Давайте вернемся к выбросам. Мы увидели, что они очень сильно портят восприятие данных аналитиком. \n",
    "\n",
    "Выбросы - это значения, которые намного превышают следующие ближайшие точки данных.\n",
    "\n",
    "Есть два типа выбросов:\n",
    "- Одномерные выбросы: Одномерные выбросы - это точки данных, значения которых выходят за пределы диапазона ожидаемых значений, основанных на одной переменной.\n",
    "- Многовариантные выбросы: при построении данных некоторые значения одной переменной могут не выходить за пределы ожидаемого диапазона, но при отображении данных с какой-либо другой переменной эти значения могут находиться далеко от ожидаемого значения.\n",
    "\n",
    "Аналитик основываясь на своем опыте должен принять решение, как поступить с выбросами. Заменить, удалить или оставить. В нашем случае, мы удалим выбросы.\n",
    "\n",
    "Следующие операции выполним последовательно.\n",
    "\n",
    "```python\n",
    "df[col].describe()\n",
    "\n",
    "df['Итоговый суммарный ущерб (тыс.руб.)'].hist(figsize=(14,4), bins=50)\n",
    "\n",
    "sns.boxplot(df[df['Итоговый суммарный ущерб (тыс.руб.)']<50]['Итоговый суммарный ущерб (тыс.руб.)']);\n",
    "\n",
    "df[df['Итоговый суммарный ущерб (тыс.руб.)']<50]['Итоговый суммарный ущерб (тыс.руб.)'].hist(figsize=(14,4), bins=50);\n",
    "\n",
    "df.drop(df[df['Итоговый суммарный ущерб (тыс.руб.)']>50].index, inplace=True, axis='index')\n",
    "```\n",
    "Проделаем такие же операции и с другими параметрами. Но, например, в случае общего времени задержки, всем выбросам присвоим некоторое максимальное значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаляем выбросы\n",
    "\n",
    "На всякий случай удаление ключевых выбросов собрано в одном месте.\n",
    "\n",
    "```python\n",
    "df.drop(np.where(df['Итоговый суммарный ущерб (тыс.руб.)']>30000)[0], inplace=True)\n",
    "df=df[df['Общее время задержки']<18]\n",
    "df=df[df['Количество задержанных поездов']<73000]\n",
    "df=df[df['Время до регистрации']<100000]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормирование данные<a class=\"anchor\" id=\"3-1\"></a>\n",
    "\n",
    "Для большинства алгоритмов машинного обучения надо выполнить нормирование данные. Нормирование данных заключается в приведении диапазона изменения значений признаков к некоторым требуемым границам (например, от 0 до 1). Нормирование является необходимым начальным этапом обработки данных при использовании многих многомерных статистических методов — снижения размерности признакового пространства, классификации и т.д., особенно если переменные измерены в шкалах, существенно различающихся в величинах (например, от миллиметров до километров).\n",
    "\n",
    "Воспользуемся препроцессиногом библиотеки sklearn.\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a=np.array([[random.randint(0,10), random.randint(0,30)] for z in range(20)])\n",
    "a\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler<a class=\"anchor\" id=\"4-1\"></a>\n",
    "\n",
    "MinMaxScaler лучше всего подходит для отображения данных и их использования при той же кластеризации.\n",
    "\n",
    "```python\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "minmax = min_max_scaler.fit_transform(a)\n",
    "minmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler<a class=\"anchor\" id=\"4-2\"></a>\n",
    "\n",
    "StandardScaler лучше подходит для нормирования в рамках алгоритмов машинного обучения.\n",
    "\n",
    "```python\n",
    "scaler = preprocessing.StandardScaler().fit(a)\n",
    "standart=scaler.fit_transform(a)\n",
    "standart\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RobustScaler<a class=\"anchor\" id=\"4-3\"></a>\n",
    "\n",
    "RobustScaler лучше работает с зашумленными данными и выбросами.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust = RobustScaler().fit_transform(a)\n",
    "robust\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Различия в способах нормирования<a class=\"anchor\" id=\"4-4\"></a>\n",
    "\n",
    "Для того, чтобы лучше понять, как работают различные способы нормирования, отобразим их на графике.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f=np.array([a, standart, minmax, robust])\n",
    "label=['Оригинальные', 'Standart', 'MinMax', 'Robust']\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 4))\n",
    "plt.title(\"title\")\n",
    "for i, ax in enumerate(axs):\n",
    "    x=f[i,:,0]\n",
    "    y=f[i,:,1]\n",
    "    ax.title.set_text(label[i])\n",
    "    ax.scatter(x, y)\n",
    "    ax.grid()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или более наглядно.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "m=[]\n",
    "\n",
    "for i in range(len(f)):\n",
    "    for j in f[i]:\n",
    "        m.append([j[0], j[1], label[i]])\n",
    "dfG=pd.DataFrame(m, columns=['x','y','standart'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.scatterplot(x='x', y='y', hue='standart', data=dfG) \n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация<a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "Обучение без учителя (unsupervised learning, неконтролируемое обучение) – класс методов машинного обучения для поиска шаблонов в наборе данных. Данные, получаемые на вход таких алгоритмов, обычно не размечены, то есть передаются только входные переменные X без соответствующих меток Y. Если в контролируемом обучении (обучении с учителем, supervised learning) система пытается извлечь уроки из предыдущих примеров, то в обучении без учителя система старается самостоятельно найти шаблоны непосредственно из приведенного примера.\n",
    "\n",
    "Методы кластеризации данных являются одним из наиболее популярных семейств машинного обучения без учителя. Рассмотрим некоторые из них подробнее. \n",
    "\n",
    "![Классическая картинка](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png)\n",
    "\n",
    "\n",
    "[Описание алгоритмов](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "# Кластеризация методом Kmeans<a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "Иерархическая кластеризация хуже подходит для кластеризации больших объемов данных в сравнении с методом k-средних. Это объясняется тем, что временная сложность алгоритма линейна для метода k-средних (O(n)) и квадратична для метода иерархической кластеризации (O(n2)).\n",
    "\n",
    "В кластеризации при помощи метода k-средних алгоритм начинает построение с произвольного выбора начальных точек, поэтому результаты, генерируемые при многократном запуске алгоритма, могут отличаться. В то же время в случае иерархической кластеризации результаты воспроизводимы.\n",
    "\n",
    "Из центроидной геометрии построения метода k-средних следует, что метод хорошо работает, когда форма кластеров является гиперсферической (например, круг в 2D или сфера в 3D).\n",
    "\n",
    "Метод k-средних более чувствителен к зашумленным данным, чем иерархический метод.\n",
    "\n",
    "## Метод локтя<a class=\"anchor\" id=\"6-1\"></a>\n",
    "\n",
    "Если истинная метка заранее не известна(как в вашем случае), то K-Means clustering можно оценить с помощью критерия локтя или коэффициента силуэта.\n",
    "\n",
    "Идея метода локтя состоит в том , чтобы выполнить кластеризацию k-средних по заданному набору данных для диапазона значений k ( num_clusters, например k=1-10) и для каждого значения k вычислить сумму квадратов ошибок (SSE).\n",
    "\n",
    "После этого постройте линейный график SSE для каждого значения k. Мы хотим свести к минимуму SSE. SSE имеет тенденцию уменьшаться к 0, когда мы увеличиваем k (и SSE равно 0, когда k равно числу точек данных в наборе данных, потому что тогда каждая точка данных является своим собственным кластером, и нет никакой ошибки между ней и центром ее кластера).\n",
    "\n",
    "Таким образом, цель состоит в том, чтобы выбрать наименьшее значение k , который все еще имеет низкий SSE, и локоть обычно представляет, где мы начинаем иметь убывающую отдачу при увеличении k.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X=df[col].values ### переменую после нормирования\n",
    "\n",
    "sse = {}\n",
    "for k in range(1, 20):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X)\n",
    "    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Количество кластеров\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод Коэффициента Силуэта<a class=\"anchor\" id=\"6-2\"></a>\n",
    "\n",
    "Более высокая оценка коэффициента силуэта относится к модели с более четко определенными кластерами. Коэффициент «силуэт» вычисляется с помощью среднего внутрикластерного расстояния (a) и среднего расстояния до ближайшего кластера (b) по каждому образцу. Силуэт вычисляется как (b - a) / max(a, b).  b — это расстояние между a и ближайшим кластером, в который a не входит. Можно вычислить среднее значение силуэта по всем образцам и использовать его как метрику для оценки количества кластеров.\n",
    "\n",
    "Более высокий коэффициент силуэта указывает на то, что объект хорошо подобран к своему собственному кластеру и плохо подобран к соседним кластерам.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for n_cluster in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=n_cluster).fit(X)\n",
    "    label = kmeans.labels_\n",
    "    sil_coeff = silhouette_score(X, label, metric='euclidean')\n",
    "    print(\"Для n_clusters={}, коэффициент силуэта {}\".format(n_cluster, sil_coeff))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "Пример выполнения кластеризации и визуализации с отображением центров кластеров.\n",
    "\n",
    "```python\n",
    "kmeans = KMeans(n_clusters = 4).fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y_kmeans, s = 20, cmap = 'summer')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c = 'blue', s = 100, alpha = 0.9);\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Обработка результатов происходит также как и в случае иерархической кластеризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дешифровка анализа<a class=\"anchor\" id=\"6-3\"></a>\n",
    "\n",
    "Попробуем объяснить кластера. Создадим копию DataFrame и сформируем столбец с номерами кластеров.\n",
    "\n",
    "```python\n",
    "df_result=df[col].copy()\n",
    "df_result['kmeans']=kmeans.labels_\n",
    "df_result.sample(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгруппируем значения вокруг кластеров. \n",
    "\n",
    "```python\n",
    "df_analize=df_result.groupby('kmeans')[col].mean()\n",
    "df_analize['Count']=df_result.groupby('kmeans')['kmeans'].count()\n",
    "df_analize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация результатов анализа<a class=\"anchor\" id=\"6-4\"></a>\n",
    "\n",
    "Построим диаграмму рассеяния.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=col[2] #Изменяйте столбцы \n",
    "y=col[3]\n",
    "print(x,y)\n",
    "sns.lmplot( x=x, y=y, data=df_result, fit_reg=False, hue='kmeans', legend=False)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже самое, но каждый график отдельно по кластеру.\n",
    "\n",
    "```python\n",
    "x=col[1] #Изменяйте столбцы \n",
    "y=col[2]\n",
    "g = sns.lmplot(x=x, y=y, hue=\"kmeans\", col=\"kmeans\",\n",
    "               data=df_result, height=6, aspect=.4, x_jitter=.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Укрупним графики.\n",
    "\n",
    "```python\n",
    "g = sns.lmplot(x=x, y=y, hue=\"kmeans\", col=\"kmeans\",\n",
    "               data=df_result, col_wrap=2, height=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN<a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход он просит уже знакомую матрицу близости и два параметра — радиус -окрестности и количество соседей. \n",
    "\n",
    "Принцип работы метода иллюстрируется анимированной картинкой.\n",
    "\n",
    "![DBSCAN](https://ichi.pro/assets/images/max/724/0*PQTm0HWuxQHuxHyl.gif)\n",
    "\n",
    "\n",
    "[Подробнее](https://habr.com/ru/post/322034/)\n",
    "\n",
    "Реализация алгоритма крайне проста с использованием библиотек. Но надо указать два параметра, минимальное количество объектов в кластере (определяем , опреедляем уровень шума) и эпсилон(eps) - радиус вокруг точки для определения соседа в кластере. \n",
    "\n",
    "Используем для расчета метод колена. Цель состоит в том, чтобы найти среднее расстояние для каждой точки до ее K ближайших соседей и выбрать расстояние, на котором происходит максимальная кривизна или резкое изменение кривой на графике.\n",
    "\n",
    "```python\n",
    "X=df[col].values ### переменую после нормирования\n",
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=5)\n",
    "nearest_neighbors.fit(X)\n",
    "distances, indices = nearest_neighbors.kneighbors(X)\n",
    "distances = np.sort(distances, axis=0)[:, 1]\n",
    "#print(distances)\n",
    "plt.plot(distances)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним саму кластеризацию.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#eps - радиус\n",
    "#минимальное количество участников в кластере\n",
    "db = DBSCAN(eps=0.65, min_samples=5).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "db.labels_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отрисуем график.\n",
    "\n",
    "```python\n",
    "colors = ['royalblue', 'maroon', 'forestgreen', 'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy']\n",
    "vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n",
    "plt.scatter(X[:,0], X[:,2], c=vectorizer(db.labels_))\n",
    "```\n",
    "\n",
    "Чтобы получить количество кластеров, надо выполнить `db.labels_.max()+1`.\n",
    "\n",
    "Обработка результатов проходит аналогично другим видам кластеризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ассоциативные правила<a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "Ассоциативные правила позволяют находить закономерности между связанными событиями. Примером такой закономерности служит правило, указывающее, что из события X следует событие Y с некоторой вероятностью. Установление таких зависимостей дает возможность находить очень простые и интуитивно понятные правила.\n",
    "\n",
    "```python\n",
    "import sqlalchemy\n",
    "\n",
    "engine = sqlalchemy.create_engine(\n",
    "                \"mysql+pymysql://root:%s@159.69.219.206:3307/rzd\" %(sql_pass), encoding='utf8', convert_unicode=True\n",
    "            )\n",
    "\n",
    "#прочитаем таблицу\n",
    "with engine.connect() as session:\n",
    "    df=pd.read_sql('SELECT * FROM grdp LIMIT 1000', con=session)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем подготовительные работы.\n",
    "\n",
    "```python\n",
    "#выделю колонки, которые хочу проанализировать\n",
    "col=['Продолжительность', 'Характер', 'Телеграмма', 'Деффект']\n",
    "#удалим все значение None\n",
    "df.dropna(subset=col, axis='index', inplace=True)\n",
    "#выполним приведение типа к строковому\n",
    "df['Продолжительность']=df['Продолжительность'].astype(str)\n",
    "#сформирую список событий\n",
    "transactions=df[col].values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним сам расчет.\n",
    "\n",
    "```python\n",
    "#загрузим пакеты, необходимые для выполнения анализа\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_as = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "#параметры можно регулировать, например, сразу отсечь все мало встречаемое\n",
    "frequent_itemsets = apriori(df_as, min_support=0.2, use_colnames=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И выведем результаты c фильтром по уровню поддержки\n",
    "\n",
    "```python\n",
    "#сгенерируем ассоциативные правила с уровнем доверия 0.1 Стоит учитывать, что данный уровень поддержки крайне\n",
    "#низкий и используется только для примера\n",
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С фильтром по уровню независимости. \n",
    "\n",
    "```python\n",
    "#найдем ассоциативные правила с уровнем независимости больше 1.1\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.1)\n",
    "rules\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расшифровка важных для нас параметров:\n",
    "- antecedents - посыл\n",
    "- consequents - последствия\n",
    "- antecedent support - встречаемость посыла\n",
    "- consequent support - встречаемость последствия\n",
    "- support - совместная встречаемость\n",
    "- confidence - вероятность появления последствия при наличии посыла\n",
    "- lift - условно мера случайности. Если значение меньше единицы, правилу доверять не стоит\n",
    "- leverage - это разность между наблюдаемой частотой, с которой условие и\n",
    "следствие появляются совместно (т.е., поддержкой ассоциации), и\n",
    "произведением частот появления (поддержек) условия и следствия по\n",
    "отдельности. \n",
    "- Conviction - В общем виде Conviction — это «частотность ошибок» нашего правила. Должно быть больше 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
