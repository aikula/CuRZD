{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ данных и визуализация информации\n",
    "\n",
    "## Подготовка данных\n",
    "\n",
    "Ниже приведен код, который упрощает повторную обработку датасета. Данный набор данных мы будем использовать дальше в учебных целях.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "\n",
    "engine = sqlalchemy.create_engine(\n",
    "                \"mysql+pymysql://root:__PASS__@__IP__6:3307/rzd\", encoding='utf8', convert_unicode=True\n",
    "            )\n",
    "\n",
    "with engine.connect() as session:\n",
    "    df=pd.read_sql('SELECT * FROM ku_asrb LIMIT 1000', con=session)\n",
    "    \n",
    "columns_date=['Дата события', 'Дата создания события в системе AC PБ', ]\n",
    "columns_float=[ 'Размер возмещенного ущерба (тыс.руб.)',\n",
    "               'Итоговый суммарный ущерб (тыс.руб.)']\n",
    "columns_int=['Погибло всего', 'Погибло пассажиров',\n",
    "       'Погибло сторонних', 'Погибло прочих',\n",
    "       'Получили тяжкие телесные повреждения: сотрудников ОАО \"РЖД\"',\n",
    "       'Получили тяжкие телесные повреждения: пассажиров',\n",
    "       'Получили тяжкие телесные повреждения: сторонние',\n",
    "       'Получили тяжкие телесные повреждения: прочие', 'Ранено легко', \n",
    "       'Станция/перегон id', 'Переезд', 'Путь общего/необщего пользования',\n",
    "       'Номер пути', 'Километр', 'Пикет', 'Общее время задержки']\n",
    "columns_time=['Время полного перерыва движения', 'Время расстройства маневровой работы',\n",
    "              'Количество задержанных поездов']\n",
    "\n",
    "def ddate(s):\n",
    "    try:\n",
    "        res=pd.to_datetime(s, format='%d%b%Y:%H:%M:%S')\n",
    "    except:\n",
    "        res=np.NaN\n",
    "    return res\n",
    "\n",
    "for i in columns_date:\n",
    "    df[i]=df[i].apply(lambda x: ddate(x))\n",
    "    \n",
    "for i in columns_float:\n",
    "    df.loc[df[i]=='.', i]='0'\n",
    "    df[i]=df[i].astype(float)\n",
    "    \n",
    "for i in columns_int:\n",
    "    df[i].fillna('.', inplace=True)\n",
    "    df.loc[df[i]=='.', i]='0'\n",
    "    df[i]=df[i].astype(int)\n",
    "    \n",
    "def time_to_sec(s):\n",
    "    s3=s.split(':')\n",
    "    if len(s3)<3:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(s3[0])*60*60+int(s3[1])*60+int(s3[2])\n",
    "    \n",
    "for i in columns_time:\n",
    "    df[i]=df[i].apply(lambda x: time_to_sec(x))\n",
    "    \n",
    "df['Время до регистрации']=df.apply(lambda x:(x['Дата создания события в системе AC PБ']-x['Дата события']).total_seconds(), axis=1)\n",
    "\n",
    "df.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разведывательный анализ данных EDA\n",
    "\n",
    "Выберем несколько колонок для примера.\n",
    "\n",
    "```python\n",
    "col=['Итоговый суммарный ущерб (тыс.руб.)', 'Общее время задержки',\n",
    "       'Количество задержанных поездов', 'Время до регистрации']\n",
    "df[col].describe()\n",
    "```\n",
    "\n",
    "Построим гистограммы.\n",
    "\n",
    "```python\n",
    "df[col].hist(figsize=(15,5), bins=40);\n",
    "```\n",
    "\n",
    "Выведем события выше некоторого уровня, чтобы более внимательно их изучить.\n",
    "\n",
    "```python\n",
    "df[df['Общее время задержки']>30]\n",
    "```\n",
    "\n",
    "Строки с большими значениями (выбросы), могут мешать рассмотреть тенденции в основном массиве данных. Их стоит часто удалять или заменить на более приемлемые значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько графиков для визуализации данных.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(data=df[col], palette='rainbow', orient='h');\n",
    "```\n",
    "\n",
    "Парные материцы рассеяния.\n",
    "\n",
    "```python\n",
    "sns.pairplot(df[col],height=3);\n",
    "```\n",
    "\n",
    "Укрупненный срез.\n",
    "\n",
    "```python\n",
    "sns.jointplot(x='Общее время задержки', y='Итоговый суммарный ущерб (тыс.руб.)', data=df, kind='scatter');\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При разведочном анализи стоит посмотреть на взаимосвязь между переменными. А именно, на корреляцию.\n",
    "\n",
    "```python\n",
    "df[col].corr()\n",
    "```\n",
    "\n",
    "И построить тепловую карту для лучшего восприятия.\n",
    "\n",
    "```python\n",
    "corr = df[col].corr()\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(corr[(corr >= 0.3) | (corr <= -0.3)],\n",
    "            cmap=\"RdBu_r\", vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 8}, square=True);\n",
    "```\n",
    "\n",
    "Вариант тепловой карты с группировкой по наиболее тесной взаимосвязи.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(14,14))\n",
    "sns.clustermap(df[col].corr())\n",
    "```\n",
    "\n",
    "Помимо корреляции надо обращать внимание и на размах величины, а именно на ее коэффициент вариации.\n",
    "\n",
    "```python\n",
    "from scipy.stats import variation\n",
    "pd.DataFrame(variation(df[col]), index=col)\n",
    "````\n",
    "\n",
    "Более нагляно об идее.\n",
    "\n",
    "```python\n",
    "df2=df[col]/df[col].max()\n",
    "df2.plot(figsize=(17,5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При формировании гипотез полезно посмотреть и на группировки по различным качественным признакам, понять как они влияют на показатели.\n",
    "\n",
    "```python\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "df.groupby('Код дороги')[col].mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка выбросов\n",
    "\n",
    "Давайте вернемся к выбросам. Мы увидели, что они очень сильно портят восприятие данных аналитиком. \n",
    "\n",
    "Выбросы - это значения, которые намного превышают следующие ближайшие точки данных.\n",
    "\n",
    "Есть два типа выбросов:\n",
    "- Одномерные выбросы: Одномерные выбросы - это точки данных, значения которых выходят за пределы диапазона ожидаемых значений, основанных на одной переменной.\n",
    "- Многовариантные выбросы: при построении данных некоторые значения одной переменной могут не выходить за пределы ожидаемого диапазона, но при отображении данных с какой-либо другой переменной эти значения могут находиться далеко от ожидаемого значения.\n",
    "\n",
    "Аналитик основываясь на своем опыте должен принять решение, как поступить с выбросами. Заменить, удалить или оставить. В нашем случае, мы удалим выбросы.\n",
    "\n",
    "Следующие операции выполним последовательно.\n",
    "\n",
    "```python\n",
    "df[col].describe()\n",
    "\n",
    "df['Итоговый суммарный ущерб (тыс.руб.)'].hist(figsize=(14,4), bins=50)\n",
    "\n",
    "sns.boxplot(df[df['Итоговый суммарный ущерб (тыс.руб.)']<50]['Итоговый суммарный ущерб (тыс.руб.)']);\n",
    "\n",
    "df[df['Итоговый суммарный ущерб (тыс.руб.)']<50]['Итоговый суммарный ущерб (тыс.руб.)'].hist(figsize=(14,4), bins=50);\n",
    "\n",
    "df.drop(df[df['Итоговый суммарный ущерб (тыс.руб.)']>50].index, inplace=True, axis='index')\n",
    "```\n",
    "Проделаем такие же операции и с другими параметрами. Но, например, в случае общего времени задержки, всем выбросам присвоим некоторое максимальное значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормирование данные\n",
    "\n",
    "Для большинства алгоритмов машинного обучения надо выполнить нормирование данные. Нормирование данных заключается в приведении диапазона изменения значений признаков к некоторым требуемым границам (например, от 0 до 1). Нормирование является необходимым начальным этапом обработки данных при использовании многих многомерных статистических методов — снижения размерности признакового пространства, классификации и т.д., особенно если переменные измерены в шкалах, существенно различающихся в величинах (например, от миллиметров до километров).\n",
    "\n",
    "Воспользуемся препроцессиногом библиотеки sklearn.\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a=np.array([[random.randint(0,10) for i in range(2)] for z in range(20)])\n",
    "a\n",
    "```\n",
    "\n",
    "МинМакс скаллерл лучше всего подходит для отображения данных и их использования при той же кластеризации.\n",
    "\n",
    "```python\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "minmax = min_max_scaler.fit_transform(a)\n",
    "minmax\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler лучше подходжит для номирования в рамках алгоритмов машинного обучения. \n",
    "\n",
    "```python\n",
    "scaler = preprocessing.StandardScaler().fit(a)\n",
    "standart=scaler.fit_transform(a)\n",
    "standart\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RobustScaler лучше работает с зашумленными данными и выбросами.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust = RobustScaler().fit_transform(a)\n",
    "robust\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы лучше понять, как работают различные способы нормирования, отобразим их на графике.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f=np.array([a, standart, minmax, robust])\n",
    "label=['Оригинальные', 'Standart', 'MinMax', 'Robust']\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 4))\n",
    "plt.title(\"title\")\n",
    "for i, ax in enumerate(axs):\n",
    "    x=f[i,:,0]\n",
    "    y=f[i,:,1]\n",
    "    ax.title.set_text(label[i])\n",
    "    ax.scatter(x, y)\n",
    "    ax.grid()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или более наглядно.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "m=[]\n",
    "\n",
    "for i in range(len(f)):\n",
    "    for j in f[i]:\n",
    "        m.append([j[0], j[1], label[i]])\n",
    "dfG=pd.DataFrame(m, columns=['x','y','standart'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.scatterplot(x='x', y='y', hue='standart', data=dfG) \n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация\n",
    "\n",
    "Обучение без учителя (unsupervised learning, неконтролируемое обучение) – класс методов машинного обучения для поиска шаблонов в наборе данных. Данные, получаемые на вход таких алгоритмов, обычно не размечены, то есть передаются только входные переменные X без соответствующих меток y. Если в контролируемом обучении (обучении с учителем, supervised learning) система пытается извлечь уроки из предыдущих примеров, то в обучении без учителя система старается самостоятельно найти шаблоны непосредственно из приведенного примера.\n",
    "\n",
    "Методы кластеризации данных являются одним из наиболее популярных семейств машинного обучения без учителя. Рассмотрим некоторые из них подробнее. Начнем с иерархиеской кластеризации.\n",
    "\n",
    "## Иерархическая кластеризация \n",
    "\n",
    "Иерархическая кластеризация (также графовые алгоритмы кластеризации и иерархический кластерный анализ) — совокупность алгоритмов упорядочивания данных, направленных на создание иерархии (дерева) вложенных кластеров. Выделяют два класса методов иерархической кластеризации:\n",
    "\n",
    "- Агломеративные методы (англ. agglomerative): новые кластеры создаются путем объединения более мелких кластеров и, таким образом, дерево создается от листьев к стволу;\n",
    "- Дивизивные или дивизионные методы (англ. divisive): новые кластеры создаются путем деления более крупных кластеров на более мелкие и, таким образом, дерево создается от ствола к листьям.\n",
    "\n",
    "Алгоритмы иерархической кластеризации предполагают, что анализируемое множество объектов характеризуется определённой степенью связности. По количеству признаков иногда выделяют монотетические и политетические методы классификации. Как и большинство визуальных способов представления зависимостей графы быстро теряют наглядность при увеличении числа кластеров.\n",
    "\n",
    "Выполним построение дендрограммы методом агломеративной кластеризации.\n",
    "\n",
    "```python\n",
    "X = df[col] #какие данные возьмем для кластеризации\n",
    "#Желательно брать данные после нормализации данных\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # создадим матрицу связей для построения дендрограммы\n",
    "\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Передаем данные для построения дендрограммы\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "# устанавливаем distance_threshold=0 чтиобы гарантированно посчитать полное дерево\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.figure(figsize=(15,5)) #размер фигуры\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# можно установить количество уровней дендрограммы, параметр р\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Количество точек в узле (или индекс точки, если нет скобок).\")\n",
    "plt.show()\n",
    "```\n",
    "Данная модель наглядна, но на практике чаще используют другой подход.\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "Z = linkage(X, method='ward') #другие методы {“ward”, “complete”, “average”, “single”}, default=”ward”\n",
    "plt.figure(figsize=(15,7))\n",
    "dendrogram(Z, truncate_mode='level')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Нам нужна матрица linkage (связей).\n",
    "\n",
    "```python\n",
    "# максимизируем количество кластеров (параметр задаем)\n",
    "max_clusters=fcluster(Z, 10, criterion='maxclust')\n",
    "max_clusters[:10]\n",
    "```\n",
    "\n",
    "```python\n",
    "#используем в качестве критерия расстояние\n",
    "d_clusters=fcluster(Z,  t=50000, criterion='distance')\n",
    "d_clusters[:10]\n",
    "```\n",
    "\n",
    "Не всегда просто интерпретировать полученные кластеры.\n",
    "\n",
    "```python\n",
    "df_result=df[col].copy()\n",
    "df_result['max_clusters']=max_clusters\n",
    "df_result['distance']=d_clusters\n",
    "df_result.sample(10)\n",
    "```\n",
    "\n",
    "Выполним группировку по кластерам.\n",
    "```python\n",
    "df_analize=df_result.groupby('max_clusters')[col].mean()\n",
    "df_analize['Count']=df_result.groupby('max_clusters')['max_clusters'].count()\n",
    "df_analize\n",
    "```\n",
    "\n",
    "Также полезно визуализировать результаты.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "x=col[0] #Изменяйте столбцы \n",
    "y=col[2]\n",
    "print(x,y)\n",
    "sns.lmplot( x=x, y=y, data=df_result, fit_reg=False, hue='max_clusters', legend=False)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Лучше работает при небольшом наборе кластеров.\n",
    "\n",
    "```python\n",
    "g = sns.lmplot(x=x, y=y, hue=\"max_clusters\", col=\"max_clusters\",\n",
    "               data=df_result, height=6, aspect=.4, x_jitter=.1)\n",
    "```\n",
    "\n",
    "Есть вариант в две колонки\n",
    "\n",
    "```python\n",
    "g = sns.lmplot(x=x, y=y, hue=\"max_clusters\", col=\"max_clusters\",\n",
    "               data=df_result, col_wrap=2, height=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация методом Kmeans\n",
    "\n",
    "Иерархическая кластеризация хуже подходит для кластеризации больших объемов данных в сравнении с методом k-средних. Это объясняется тем, что временная сложность алгоритма линейна для метода k-средних (O(n)) и квадратична для метода иерархической кластеризации (O(n2)).\n",
    "\n",
    "В кластеризации при помощи метода k-средних алгоритм начинает построение с произвольного выбора начальных точек, поэтому результаты, генерируемые при многократном запуске алгоритма, могут отличаться. В то же время в случае иерархической кластеризации результаты воспроизводимы.\n",
    "\n",
    "Из центроидной геометрии построения метода k-средних следует, что метод хорошо работает, когда форма кластеров является гиперсферической (например, круг в 2D или сфера в 3D).\n",
    "\n",
    "Метод k-средних более чувствителен к зашумленным данным, чем иерархический метод.\n",
    "\n",
    "### Метод локтя\n",
    "\n",
    "Если истинная метка заранее не известна(как в вашем случае), то K-Means clustering можно оценить с помощью критерия локтя или коэффициента силуэта.\n",
    "\n",
    "Идея метода локтя состоит в том , чтобы выполнить кластеризацию k-средних по заданному набору данных для диапазона значений k ( num_clusters, например k=1-10) и для каждого значения k вычислить сумму квадратов ошибок (SSE).\n",
    "\n",
    "После этого постройте линейный график SSE для каждого значения k. Мы хотим свести к минимуму SSE. SSE имеет тенденцию уменьшаться к 0, когда мы увеличиваем k (и SSE равно 0, когда k равно числу точек данных в наборе данных, потому что тогда каждая точка данных является своим собственным кластером, и нет никакой ошибки между ней и центром ее кластера).\n",
    "\n",
    "Таким образом, цель состоит в том, чтобы выбрать наименьшее значение k , который все еще имеет низкий SSE, и локоть обычно представляет, где мы начинаем иметь убывающую отдачу при увеличении k.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X=df[col].values ### переменую после нормирования\n",
    "\n",
    "sse = {}\n",
    "for k in range(1, 20):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X)\n",
    "    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Количество кластеров\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Метод Коэффициента Силуэта:\n",
    "\n",
    "Более высокая оценка коэффициента силуэта относится к модели с более четко определенными кластерами. Коэффициент силуэта определяется для каждой выборки и состоит из двух баллов:\n",
    "- a: среднее расстояние между образцом и всеми другими точками того же класса. \n",
    "- b: среднее расстояние между образцом и всеми другими точками в следующей точке ближайший кластер.\n",
    "\n",
    "Коэффициент силуэта для одного образца затем задается как:\n",
    "\n",
    "s=b-a/max(a,b)\n",
    "\n",
    "Теперь, чтобы найти оптимальное значение k для KMeans, выполните цикл через 1..n для n_clusters в KMeans и вычислите коэффициент силуэта для каждой выборки.\n",
    "\n",
    "Более высокий коэффициент силуэта указывает на то, что объект хорошо подобран к своему собственному кластеру и плохо подобран к соседним кластерам.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for n_cluster in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=n_cluster).fit(X)\n",
    "    label = kmeans.labels_\n",
    "    sil_coeff = silhouette_score(X, label, metric='euclidean')\n",
    "    print(\"Для n_clusters={}, коэффициент силуэта {}\".format(n_cluster, sil_coeff))\n",
    "```\n",
    "\n",
    "Пример выполнения кластеризации и визуализации с отображением центров кластеров.\n",
    "\n",
    "```python\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y_kmeans, s = 20, cmap = 'summer')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c = 'blue', s = 100, alpha = 0.9);\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Обработка результатов происходит также как и в случае иерархической кластеризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход он просит уже знакомую матрицу близости и два параметра — радиус -окрестности и количество соседей. \n",
    "![Классическая картинка](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png)\n",
    "\n",
    "\n",
    "[Описание алгоритмов](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "[Подробнее](https://habr.com/ru/post/322034/)\n",
    "\n",
    "Реализация алгоритма крайне проста с использованием библиотек. \n",
    "```python\n",
    "X=df[col] #заменяем переменную\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# eps - радиус\n",
    "# минимальное количество участников в кластере\n",
    "db = DBSCAN(eps=1, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "db.labels_\n",
    "```\n",
    "\n",
    "Отрисуем график.\n",
    "\n",
    "```python\n",
    "colors = ['royalblue', 'maroon', 'forestgreen', 'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy']\n",
    "vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n",
    "plt.scatter(X[:,0], X[:,2], c=vectorizer(db.labels_))\n",
    "```\n",
    "\n",
    "Чтобы получить количество кластеров, надо выполнить `db.labels_.max()`.\n",
    "\n",
    "Обработка результатов проходит аналогично другим видам кластеризации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Снижение размерности\n",
    "\n",
    "Одной из самых очевидных задач, которые возникают в отсутствие явной разметки, является задача снижения размерности данных. С одной стороны её можно рассматривать как помощь в визуализации данных, c другой стороны подобное снижение размерности может убрать лишние сильно скоррелированные признаки у наблюдений и подготовить данные для дальнейшей обработки в режиме обучения с учителем, например сделать входные данные более \"перевариваемыми\" для деревьев решений.\n",
    "\n",
    "Объемы и сложность данных постоянно растут. В результате, существенно увеличивается и их размерность. Для компьютеров это не проблема — в отличие от людей: мы ограничены всего тремя измерениями.\n",
    "\n",
    "Структура, скрытая в данных, может быть восстановлена только с помощью специальных математических методов. К ним относится подраздел машинного обучения без учителя под названием множественное обучение (manifold learning) или нелинейное уменьшение размерности (nonlinear dimensionality reduction).\n",
    "\n",
    "## PCA\n",
    "\n",
    "PCA помогает выразить несколько признаков через один, что позволяет работать с более простой моделью.\n",
    "\n",
    "PCA аппроксимирует n-размерное облако наблюдений до эллипсоида (тоже n-мерного), полуоси которого и будут являться будущими главными компонентами. И при проекции на такие оси (снижении размерности) сохраняется наибольшее количество информации.\n",
    "\n",
    "Вычисление главных компонент сводится к вычислению собственных векторов и собственных значений ковариационной матрицы исходных данных или к сингулярному разложению матрицы данных.\n",
    "\n",
    "```python\n",
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(df[col])\n",
    "\n",
    "x_axis = X_pca[:, 0]\n",
    "y_axis = X_pca[:, 1]\n",
    "\n",
    "plt.scatter(x_axis, y_axis, c=df_result.max_clusters)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть, как образованы главные компоненты.\n",
    "\n",
    "```python\n",
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1], [\"Первая компонента\", \"Вторая компонента\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(col), col, rotation=60, ha='left')\n",
    "plt.xlabel(\"Характеристика\")\n",
    "plt.ylabel(\"Главные компоненты\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "\n",
    "t-SNE (t-distributed stochastic neighbor embedding) — техника нелинейного снижения размерности и визуализации многомерных переменных. Этот алгоритм может свернуть сотни измерений к меньшему количеству, сохраняя при этом важные отношения между данными: чем ближе объекты располагаются в исходном пространстве, тем меньше расстояние между этими объектами в пространстве сокращенной размерности. t-SNE неплохо работает на маленьких и средних реальных наборах данных и не требует большого количества настроек гиперпараметров.\n",
    "\n",
    "Алгоритм t-SNE, который также относят к методам множественного обучения признаков, был опубликован в 2008 году голландским исследователем Лоуренсом ван дер Маатеном (сейчас работает в Facebook AI Research) и Джеффри Хинтоном. Классический SNE был предложен Хинтоном и Ровейсом в 2002. В статье 2008 года описывается несколько «трюков», которые позволили упростить процесс поиска глобальных минимумов, и повысить качество визуализации.\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Определяем модель и скорость обучения\n",
    "model = TSNE(learning_rate=100)\n",
    "\n",
    "# Обучаем модель\n",
    "transformed = model.fit_transform(rob)\n",
    "\n",
    "x_axis = transformed[:, 0]\n",
    "y_axis = transformed[:, 1]\n",
    "\n",
    "plt.scatter(x_axis, y_axis, c=df_result.max_clusters)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ассоциативные правила\n",
    "\n",
    "Ассоциативные правила позволяют находить закономерности между связанными событиями. Примером такой закономерности служит правило, указывающее, что из события X следует событие Y с некоторой вероятностью. Установление таких зависимостей дает возможность находить очень простые и интуитивно понятные правила.\n",
    "\n",
    "```python\n",
    "#прочитаем таблицу\n",
    "with engine.connect() as session:\n",
    "    df=pd.read_sql('SELECT * FROM grdp LIMIT 1000', con=session)\n",
    "```\n",
    "\n",
    "Проведем подготовительные работы.\n",
    "\n",
    "```python\n",
    "#выделю колонки, которые хочу проанализировать\n",
    "col=['Продолжительность', 'Характер', 'Телеграмма', 'Деффект']\n",
    "#удалим все значение None\n",
    "df.dropna(subset=col, axis='index', inplace=True)\n",
    "# выполним приведение типа к строковому\n",
    "df['Продолжительность']=df['Продолжительность'].astype(str)\n",
    "#сформирую список событий\n",
    "transactions=df[col].values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним сам расчет.\n",
    "\n",
    "```python\n",
    "# загрузим пакеты, необходимые для выполнения анализа\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_as = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "#параметры можно регулировать, например, сразу отсечь все мало встречаемое\n",
    "frequent_itemsets = apriori(df_as, min_support=0.2, use_colnames=True)\n",
    "```\n",
    "\n",
    "И выведем результаты c фильстром по уровню поддержки\n",
    "\n",
    "```python\n",
    "# сгенерируем ассоциативные правила с уровнем доверия 0.1 Стоит учитывать, что данный уровень поддержки крайне\n",
    "# низкий и используется только для примера\n",
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "```\n",
    "С фильтром по уровню независимости. \n",
    "\n",
    "```python\n",
    "# найдем ассоциативные правила с уровнем независимости больше 1.2\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "rules\n",
    "```\n",
    "\n",
    "Расшифровка важных для нас параметров:\n",
    "- antecedents - посыл\n",
    "- consequents - последствия\n",
    "- antecedent support - встречаемость посыла\n",
    "- consequent support - встречаемость последствия\n",
    "- support - совместная встречаемость\n",
    "- confidence - вероятность появления последствия при наличии посыла\n",
    "- lift - условно мера случайности. Если значение меньше единицы, правилу доверять не стоит\n",
    "- leverage - это разность между наблюдаемой частотой, с которой условие и\n",
    "следствие появляются совместно (т.е., поддержкой ассоциации), и\n",
    "произведением частот появления (поддержек) условия и следствия по\n",
    "отдельности. \n",
    "- Conviction - В общем виде Conviction — это «частотность ошибок» нашего правила. Должно быть больше 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
