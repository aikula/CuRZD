{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация\n",
    "\n",
    "Обучение без учителя (unsupervised learning, неконтролируемое обучение) – класс методов машинного обучения для поиска шаблонов в наборе данных. Данные, получаемые на вход таких алгоритмов, обычно не размечены, то есть передаются только входные переменные X без соответствующих меток y. Если в контролируемом обучении (обучении с учителем, supervised learning) система пытается извлечь уроки из предыдущих примеров, то в обучении без учителя система старается самостоятельно найти шаблоны непосредственно из приведенного примера.\n",
    "\n",
    "Методы кластеризации данных являются одним из наиболее популярных семейств машинного обучения без учителя. Рассмотрим некоторые из них подробнее. Начнем с иерархиеской кластеризации.\n",
    "\n",
    "## Иерархическая кластеризация \n",
    "\n",
    "Иерархическая кластеризация (также графовые алгоритмы кластеризации и иерархический кластерный анализ) — совокупность алгоритмов упорядочивания данных, направленных на создание иерархии (дерева) вложенных кластеров. Выделяют два класса методов иерархической кластеризации:\n",
    "\n",
    "- Агломеративные методы (англ. agglomerative): новые кластеры создаются путем объединения более мелких кластеров и, таким образом, дерево создается от листьев к стволу;\n",
    "- Дивизивные или дивизионные методы (англ. divisive): новые кластеры создаются путем деления более крупных кластеров на более мелкие и, таким образом, дерево создается от ствола к листьям.\n",
    "\n",
    "Алгоритмы иерархической кластеризации предполагают, что анализируемое множество объектов характеризуется определённой степенью связности. По количеству признаков иногда выделяют монотетические и политетические методы классификации. Как и большинство визуальных способов представления зависимостей графы быстро теряют наглядность при увеличении числа кластеров.\n",
    "\n",
    "Выполним построение дендрограммы методом агломеративной кластеризации.\n",
    "\n",
    "```python\n",
    "X = df[col] #какие данные возьмем для кластеризации\n",
    "#Желательно брать данные после нормализации данных\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # создадим матрицу связей для построения дендрограммы\n",
    "\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Передаем данные для построения дендрограммы\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "# устанавливаем distance_threshold=0 чтиобы гарантированно посчитать полное дерево\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.figure(figsize=(15,5)) #размер фигуры\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# можно установить количество уровней дендрограммы, параметр р\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Количество точек в узле (или индекс точки, если нет скобок).\")\n",
    "plt.show()\n",
    "```\n",
    "Данная модель наглядна, но на практике чаще используют другой подход.\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "Z = linkage(X, method='ward') #другие методы {“ward”, “complete”, “average”, “single”}, default=”ward”\n",
    "plt.figure(figsize=(15,7))\n",
    "dendrogram(Z, truncate_mode='level')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Нам нужна матрица linkage (связей).\n",
    "\n",
    "```python\n",
    "# максимизируем количество кластеров (параметр задаем)\n",
    "max_clusters=fcluster(Z, 10, criterion='maxclust')\n",
    "max_clusters[:10]\n",
    "```\n",
    "\n",
    "```python\n",
    "#используем в качестве критерия расстояние\n",
    "d_clusters=fcluster(Z,  t=50000, criterion='distance')\n",
    "d_clusters[:10]\n",
    "```\n",
    "\n",
    "Не всегда просто интерпретировать полученные кластеры.\n",
    "\n",
    "```python\n",
    "df_result=df[col].copy()\n",
    "df_result['max_clusters']=max_clusters\n",
    "df_result['distance']=d_clusters\n",
    "df_result.sample(10)\n",
    "```\n",
    "\n",
    "Выполним группировку по кластерам.\n",
    "```python\n",
    "df_analize=df_result.groupby('max_clusters')[col].mean()\n",
    "df_analize['Count']=df_result.groupby('max_clusters')['max_clusters'].count()\n",
    "df_analize\n",
    "```\n",
    "\n",
    "Также полезно визуализировать результаты.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "x=col[0] #Изменяйте столбцы \n",
    "y=col[2]\n",
    "print(x,y)\n",
    "sns.lmplot( x=x, y=y, data=df_result, fit_reg=False, hue='max_clusters', legend=False)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Лучше работает при небольшом наборе кластеров.\n",
    "\n",
    "```python\n",
    "g = sns.lmplot(x=x, y=y, hue=\"max_clusters\", col=\"max_clusters\",\n",
    "               data=df_result, height=6, aspect=.4, x_jitter=.1)\n",
    "```\n",
    "\n",
    "Есть вариант в две колонки\n",
    "\n",
    "```python\n",
    "g = sns.lmplot(x=x, y=y, hue=\"max_clusters\", col=\"max_clusters\",\n",
    "               data=df_result, col_wrap=2, height=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Желательно брать данные после нормализации данных\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Снижение размерности<a name=\"i8\"></a>\n",
    "\n",
    "Одной из самых очевидных задач, которые возникают в отсутствие явной разметки, является задача снижения размерности данных. С одной стороны её можно рассматривать как помощь в визуализации данных, c другой стороны подобное снижение размерности может убрать лишние сильно скоррелированные признаки у наблюдений и подготовить данные для дальнейшей обработки в режиме обучения с учителем, например сделать входные данные более \"перевариваемыми\" для деревьев решений.\n",
    "\n",
    "Объемы и сложность данных постоянно растут. В результате, существенно увеличивается и их размерность. Для компьютеров это не проблема — в отличие от людей: мы ограничены всего тремя измерениями.\n",
    "\n",
    "Структура, скрытая в данных, может быть восстановлена только с помощью специальных математических методов. К ним относится подраздел машинного обучения без учителя под названием множественное обучение (manifold learning) или нелинейное уменьшение размерности (nonlinear dimensionality reduction).\n",
    "\n",
    "## PCA\n",
    "\n",
    "PCA помогает выразить несколько признаков через один, что позволяет работать с более простой моделью.\n",
    "\n",
    "PCA аппроксимирует n-размерное облако наблюдений до эллипсоида (тоже n-мерного), полуоси которого и будут являться будущими главными компонентами. И при проекции на такие оси (снижении размерности) сохраняется наибольшее количество информации.\n",
    "\n",
    "Вычисление главных компонент сводится к вычислению собственных векторов и собственных значений ковариационной матрицы исходных данных или к сингулярному разложению матрицы данных.\n",
    "\n",
    "```python\n",
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(df[col])\n",
    "\n",
    "x_axis = X_pca[:, 0]\n",
    "y_axis = X_pca[:, 1]\n",
    "\n",
    "plt.scatter(x_axis, y_axis, c=df_result.max_clusters)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Можно посмотреть, как образованы главные компоненты.\n",
    "\n",
    "```python\n",
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1], [\"Первая компонента\", \"Вторая компонента\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(col), col, rotation=60, ha='left')\n",
    "plt.xlabel(\"Характеристика\")\n",
    "plt.ylabel(\"Главные компоненты\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE<a name=\"i9\"></a>\n",
    "\n",
    "t-SNE (t-distributed stochastic neighbor embedding) — техника нелинейного снижения размерности и визуализации многомерных переменных. Этот алгоритм может свернуть сотни измерений к меньшему количеству, сохраняя при этом важные отношения между данными: чем ближе объекты располагаются в исходном пространстве, тем меньше расстояние между этими объектами в пространстве сокращенной размерности. t-SNE неплохо работает на маленьких и средних реальных наборах данных и не требует большого количества настроек гиперпараметров.\n",
    "\n",
    "Алгоритм t-SNE, который также относят к методам множественного обучения признаков, был опубликован в 2008 году голландским исследователем Лоуренсом ван дер Маатеном (сейчас работает в Facebook AI Research) и Джеффри Хинтоном. Классический SNE был предложен Хинтоном и Ровейсом в 2002. В статье 2008 года описывается несколько «трюков», которые позволили упростить процесс поиска глобальных минимумов, и повысить качество визуализации.\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Определяем модель и скорость обучения\n",
    "model = TSNE(learning_rate=100)\n",
    "\n",
    "# Обучаем модель\n",
    "transformed = model.fit_transform(rob)\n",
    "\n",
    "x_axis = transformed[:, 0]\n",
    "y_axis = transformed[:, 1]\n",
    "\n",
    "plt.scatter(x_axis, y_axis, c=df_result.max_clusters)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
